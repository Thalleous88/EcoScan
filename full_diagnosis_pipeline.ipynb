{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3344a7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SEBASTIAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig, \n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForSequenceClassification\n",
    ")\n",
    "from ultralytics import YOLO\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b88f3242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO:__main__:Analyzing: laptop_line_test_img.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 Laptop, 75.0ms\n",
      "Speed: 16.9ms preprocess, 75.0ms inference, 22.8ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device_detected': 'Laptop', 'visual_condition': 'laptop_lines', 'nlp_issues': ['Display_Visual'], 'recommendation': \"Diagnosis: Laptop Lines - Diagnosis The laptop lines detected on your device indicate that there may be an issue with the display visual component, which could cause the lines to appear. The specific symptom of this problem is the presence of visible lines on the screen, indicating that the liquid crystal panel has failed. This can result from various factors such as damaged pixels, broken circuitry, or faulty components. In order to diagnose and fix this issue, you will need to replace the entire display assembly. Please refer to our troubleshooting guide for further assistance. Reaction: Replace Display Assembly If you have any doubts about whether it's time to replace the display assembly, consult our troubleshooting guide for more information. Action: Replace Display Assembly We recommend replacing the display assembly if you suspect that the laptop lines are caused by a physical damage issue. Our replacement parts come with warranties and guarantees, so feel free to choose one that best suits your needs. We hope this helps! If you require additional support, please don't hesitate to contact us at [insert contact details]. Thank you for choosing our products.\\n\"}\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    # Model Paths\n",
    "    YOLO_PATH = \"electronics_type_classifier/runs/detect/train4/weights/best.pt\"\n",
    "    CLASSIFIER_PATH = \"condition_classifier/defect_classifier_v1.pth\"\n",
    "    NLP_PATH = \"keyword_extraction/electronics_nlp_model\"\n",
    "    LLM_MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    \n",
    "    # Token (Only needed for Gemma)\n",
    "    HF_TOKEN = \"\" \n",
    "    \n",
    "    # Device\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    \n",
    "    # VISUAL CLASSIFIER CLASSES (What your ResNet knows)\n",
    "    IMG_CLASSES = [\n",
    "        'laptop_normal', 'laptop_crack', 'laptop_fades', 'laptop_lines', \n",
    "        'laptop_spot', 'phone_dead_pixel', 'phone_scratch', 'phone_crack', 'phone_normal'\n",
    "    ]\n",
    "    \n",
    "    # NLP LABELS\n",
    "    NLP_LABELS = [\n",
    "        'Power_Failure', 'Battery_Charging', 'Display_Visual', 'Audio_Sound',\n",
    "        'Overheating_Thermal', 'Connectivity_Signal', 'Water_Liquid_Damage',\n",
    "        'Mechanical_Motor', 'Input_Controls', 'Software_Error', 'Data_Storage',\n",
    "        'Sensor_Accuracy'\n",
    "    ]\n",
    "\n",
    "# Setup Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- COMPONENT 1: LLM ADVISOR ---\n",
    "class DeviceAdvisorLLM:\n",
    "    def __init__(self, model_id, hf_token):\n",
    "        logger.info(f\"Loading LLM: {model_id}...\")\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            token=hf_token\n",
    "        )\n",
    "\n",
    "    def generate_recommendation(self, device_type, visual_condition, nlp_issues):\n",
    "        visual_clean = visual_condition.replace(\"_\", \" \").title()\n",
    "        issues_clean = \", \".join([x.replace(\"_\", \" \") for x in nlp_issues]) if nlp_issues else \"None\"\n",
    "\n",
    "        # 1. Prompt with negative constraints and one-shot example\n",
    "        prompt = f\"\"\"<|system|>\n",
    "You are a technical diagnostic tool. Output the status in the exact format shown. \n",
    "Do not provide repair instructions or steps. Keep descriptions brief.\n",
    "</s>\n",
    "<|user|>\n",
    "Device: Smartphone\n",
    "Visual: Screen Crack\n",
    "Internal: Touch Issue\n",
    "\n",
    "Response:\n",
    "Diagnosis: Cracked Screen Digitizer\n",
    "Severity: Medium\n",
    "Action: Replace Screen\n",
    "Reasoning: Physical damage is interfering with touch sensors.\n",
    "</s>\n",
    "<|user|>\n",
    "Device: {device_type}\n",
    "Visual: {visual_clean}\n",
    "Internal: {issues_clean}\n",
    "\n",
    "Response:\n",
    "Diagnosis:\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        # 2. Generation with higher token limit (250)\n",
    "        outputs = self.model.generate(\n",
    "            inputs, \n",
    "            max_new_tokens=250,       \n",
    "            temperature=0.2,          \n",
    "            repetition_penalty=1.2,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        raw_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 3. Cleanup logic\n",
    "        if \"<|assistant|>\" in raw_response:\n",
    "            generated_text = raw_response.split(\"<|assistant|>\")[-1].strip()\n",
    "        else:\n",
    "            generated_text = raw_response\n",
    "\n",
    "        # Force \"Diagnosis:\" prefix if missing (since we forced it in prompt)\n",
    "        final_text = \"Diagnosis: \" + generated_text if not generated_text.startswith(\"Diagnosis:\") else generated_text\n",
    "            \n",
    "        return self._format_output(final_text)\n",
    "\n",
    "    def _format_output(self, text):\n",
    "        \"\"\"\n",
    "        Parses the raw text into a clean string even if the model messes up slightly.\n",
    "        \"\"\"\n",
    "        # Fallback parsing strategy: split by newlines and reconstruct\n",
    "        lines = text.split('\\n')\n",
    "        result = {}\n",
    "        current_key = \"Summary\"\n",
    "        \n",
    "        # Keys we expect to find\n",
    "        target_keys = [\"Diagnosis\", \"Severity\", \"Action\", \"Reasoning\"]\n",
    "        \n",
    "        for line in lines:\n",
    "            # Check if this line starts with one of our keys\n",
    "            found_key = False\n",
    "            for key in target_keys:\n",
    "                if line.strip().startswith(key + \":\"):\n",
    "                    _, val = line.split(\":\", 1)\n",
    "                    result[key] = val.strip()\n",
    "                    current_key = key\n",
    "                    found_key = True\n",
    "                    break\n",
    "            \n",
    "            # If it's a continuation of the previous line (model rambling)\n",
    "            if not found_key and current_key in result and line.strip():\n",
    "                result[current_key] += \" \" + line.strip()\n",
    "        \n",
    "        # Rebuild the final string to look clean\n",
    "        final_str = \"\"\n",
    "        for k in target_keys:\n",
    "            if k in result:\n",
    "                final_str += f\"{k}: {result[k]}\\n\"\n",
    "        \n",
    "        # If regex failed completely, just return the raw text so we don't return empty string\n",
    "        return final_str if final_str.strip() else text\n",
    "\n",
    "# --- MAIN PIPELINE CLASS ---\n",
    "class DiagnosisPipeline:\n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "        \n",
    "        # --- MEMORY CLEANUP ---\n",
    "        # Clear any leftover garbage from previous runs\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # --- 1. LOAD THE LLM FIRST ---\n",
    "        # The LLM is the \"biggest rock\". We load it first to ensure \n",
    "        # it gets the continuous VRAM block it needs for 4-bit quantization.\n",
    "        self._load_llm()\n",
    "        \n",
    "        # --- 2. LOAD SMALLER MODELS ---\n",
    "        # These fit in the gaps or can be managed more easily by PyTorch\n",
    "        self._load_yolo()\n",
    "        self._load_classifier()\n",
    "        self._load_nlp()\n",
    "\n",
    "    def _load_llm(self):\n",
    "        # We pass the token (if needed) and model ID\n",
    "        self.advisor = DeviceAdvisorLLM(self.config.LLM_MODEL_ID, self.config.HF_TOKEN)\n",
    "\n",
    "    def _load_yolo(self):\n",
    "        try:\n",
    "            self.yolo = YOLO(self.config.YOLO_PATH)\n",
    "        except:\n",
    "            logger.warning(\"Using fallback YOLOv8n\")\n",
    "            self.yolo = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "    def _load_classifier(self):\n",
    "        # RESNET50\n",
    "        self.classifier = timm.create_model('resnet50', pretrained=False, num_classes=9)\n",
    "        try:\n",
    "            state_dict = torch.load(self.config.CLASSIFIER_PATH, map_location=self.config.DEVICE)\n",
    "            self.classifier.load_state_dict(state_dict)\n",
    "            self.classifier.to(self.config.DEVICE).eval()\n",
    "        except:\n",
    "            logger.warning(\"Visual Classifier weights not found. Visual checks will be skipped.\")\n",
    "            self.classifier = None\n",
    "        \n",
    "        self.img_transforms = transforms.Compose([\n",
    "            transforms.Resize((512, 512)), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def _load_nlp(self):\n",
    "        try:\n",
    "            self.nlp_tokenizer = DistilBertTokenizer.from_pretrained(self.config.NLP_PATH)\n",
    "            self.nlp_model = DistilBertForSequenceClassification.from_pretrained(self.config.NLP_PATH)\n",
    "            self.nlp_model.to(self.config.DEVICE).eval()\n",
    "        except:\n",
    "            logger.warning(\"NLP Model not found.\")\n",
    "\n",
    "    def analyze_case(self, image_path, user_comment):\n",
    "        logger.info(f\"Analyzing: {image_path}\")\n",
    "        \n",
    "        # Defaults\n",
    "        device_type = \"Unknown Device\"\n",
    "        visual_condition = \"N/A\" # Default to N/A\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(image_path).convert(\"RGB\")\n",
    "            results = self.yolo(img)\n",
    "            \n",
    "            detected_box = None\n",
    "            \n",
    "            # 1. YOLO DETECTION\n",
    "            for box in results[0].boxes:\n",
    "                cls_id = int(box.cls)\n",
    "                device_type = self.yolo.names[cls_id] # Update device type\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                detected_box = (x1, y1, x2, y2)\n",
    "                break \n",
    "            \n",
    "            # List of devices we have a ResNet for\n",
    "            screen_devices = [\n",
    "                'Laptop', 'Smartphone', 'Flat-Panel-Monitor', 'Flat-Panel-TV', \n",
    "                'Desktop-PC', 'Digital-Oscilloscope', 'Telephone-Set'\n",
    "            ]\n",
    "            \n",
    "            # 2. ROUTING LOGIC\n",
    "            if detected_box:\n",
    "                # CASE A: It is a screen device -> Use ResNet\n",
    "                if device_type in screen_devices and self.classifier:\n",
    "                    crop = img.crop(detected_box)\n",
    "                    visual_condition = self._classify_crop(crop)\n",
    "                \n",
    "                # CASE B: Known device, but not a screen (e.g., Drone) -> Skip ResNet\n",
    "                else:\n",
    "                    visual_condition = \"N/A (Non-screen device)\"\n",
    "            \n",
    "            # CASE C: YOLO saw nothing\n",
    "            elif not detected_box:\n",
    "                logger.warning(\"YOLO: No object detected.\")\n",
    "                device_type = \"Unknown (Rely on Text)\"\n",
    "                visual_condition = \"N/A\"\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Visual pipeline error: {e}\")\n",
    "\n",
    "        # 3. NLP & LLM\n",
    "        nlp_issues = self._analyze_text(user_comment)\n",
    "\n",
    "        recommendation = self.advisor.generate_recommendation(\n",
    "            device_type=device_type,\n",
    "            visual_condition=visual_condition,\n",
    "            nlp_issues=nlp_issues\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"device_detected\": device_type,\n",
    "            \"visual_condition\": visual_condition,\n",
    "            \"nlp_issues\": nlp_issues,\n",
    "            \"recommendation\": recommendation\n",
    "        }\n",
    "\n",
    "    def _classify_crop(self, pil_img):\n",
    "        # (Your existing logic)\n",
    "        tensor = self.img_transforms(pil_img).unsqueeze(0).to(self.config.DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = self.classifier(tensor)\n",
    "            probs = F.softmax(output[0], dim=0)\n",
    "            top_prob, top_idx = torch.max(probs, 0)\n",
    "            if top_prob.item() < 0.4:\n",
    "                return \"Uncertain/Normal\"\n",
    "            return self.config.IMG_CLASSES[top_idx.item()]\n",
    "\n",
    "    def _analyze_text(self, text):\n",
    "        # (Your existing logic)\n",
    "        if not hasattr(self, 'nlp_model'): return []\n",
    "        inputs = self.nlp_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(self.config.DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.nlp_model(**inputs)\n",
    "            probs = torch.sigmoid(outputs.logits).squeeze()\n",
    "        \n",
    "        detected = []\n",
    "        for i, prob in enumerate(probs):\n",
    "            if prob > 0.5:\n",
    "                detected.append(self.config.NLP_LABELS[i])\n",
    "        return detected\n",
    "\n",
    "    def _classify_crop(self, pil_img):\n",
    "        tensor = self.img_transforms(pil_img).unsqueeze(0).to(self.config.DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = self.classifier(tensor)\n",
    "            probs = F.softmax(output[0], dim=0)\n",
    "            top_prob, top_idx = torch.max(probs, 0)\n",
    "            \n",
    "            if top_prob.item() < 0.4:\n",
    "                return \"Uncertain/Normal\"\n",
    "            return self.config.IMG_CLASSES[top_idx.item()]\n",
    "\n",
    "    def _analyze_text(self, text):\n",
    "        if not hasattr(self, 'nlp_model'): return []\n",
    "        inputs = self.nlp_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(self.config.DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.nlp_model(**inputs)\n",
    "            probs = torch.sigmoid(outputs.logits).squeeze()\n",
    "        \n",
    "        detected = []\n",
    "        for i, prob in enumerate(probs):\n",
    "            if prob > 0.5:\n",
    "                detected.append(self.config.NLP_LABELS[i])\n",
    "        return detected\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = DiagnosisPipeline()\n",
    "    # Test\n",
    "    res = pipeline.analyze_case(\"laptop_line_test_img.jpg\", \"Screen is broken with lines in it.\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf51e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
